{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mukuev_Fedorova_Kukhareva_Skovorodko_Samoylov",
      "provenance": [],
      "collapsed_sections": [
        "TATrq_yG2yFZ",
        "Ra8t5YeSU8AI",
        "1mFtXjhjUhdz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "TATrq_yG2yFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmeglfJV8D5o",
        "outputId": "e11327ce-6366-41a2-cc4a-ca128553918c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.0->statsmodels) (1.15.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 682 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "id7RfF8jiv3S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "import xgboost as xgb\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datetime import timedelta \n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Настройка автоматизированного пайплайна"
      ],
      "metadata": {
        "id": "Ra8t5YeSU8AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим функцию потерь, которую нужно минимизировать через настройку гиперпараметров моделей\n",
        "\n",
        "def calc_loss(y, y_pred, key_rate):\n",
        "  if y >= 0 and y_pred >= 0:\n",
        "    if y >= y_pred:\n",
        "      # Могли вложить в деривативы больше, пришлось профицит кидать под овернайт\n",
        "      return (key_rate - 0.009) * (y - y_pred) - (key_rate + 0.005) * (y - y_pred)\n",
        "    elif y < y_pred:\n",
        "      # Получили доп доход с деривативов, но пришлось разницу занимать под процент\n",
        "      return (key_rate + 0.005) * (y_pred - y) - (key_rate + 0.01) * (y_pred - y)\n",
        "  elif y < 0 and y_pred < 0:\n",
        "    if y >= y_pred:\n",
        "      # Заработали на овернайте, но упустили возможность вложить в деривативы\n",
        "      return (y_pred - y) * (key_rate + 0.005) + (y - y_pred) * (key_rate - 0.009)\n",
        "    elif y < y_pred:\n",
        "      # Пришлось занять под процент из-за ошибки прогноза\n",
        "      return (y - y_pred) * (key_rate + 0.01)\n",
        "  elif y >= 0 and y_pred <= 0:\n",
        "    # Заняли излишек, с учетом излишка вложили под овернайт, но упустили возможность вложить в деривативы\n",
        "    return (key_rate - 0.009) * (y - y_pred) - (key_rate + 0.005) * y\n",
        "  elif y <= 0 and y_pred >= 0:\n",
        "    # Вложили в деривативы по прогнозу, но ошиблись и пришлось много занимать\n",
        "    return (key_rate + 0.005) * y_pred - (key_rate + 0.01) * (y_pred - y)"
      ],
      "metadata": {
        "id": "-zMv-yZdBUdj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline:\n",
        "  \n",
        "  def __init__(self, data, key_rate = None, best_model = None):\n",
        "\n",
        "    # Инициализируем параметры\n",
        "    self.data = data\n",
        "    self.best_model = best_model\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    # Отбираем данные в будние дни, так как в выходные сальдо преимущественно нулевое\n",
        "    self.data = pd.DataFrame(self.data['Balance'])\n",
        "    self.data['Weekday'] = self.data.index.dayofweek + 1\n",
        "    self.data = self.data[(self.data['Weekday'] != 6) & (self.data['Weekday'] != 7)]\n",
        "\n",
        "    # Определяем даты начала и конца исходного датасета для загрузки других признаков\n",
        "    start_date = self.data.index[0]\n",
        "    end_date = self.data.index[-1]\n",
        "\n",
        "    # Загружаем некоторые признаки из yfinance: курс доллара к рублю, курс нефти, индекс мосбиржи\n",
        "    usd_rub = yf.download('RUB=X', start = start_date, end = end_date)['Adj Close']\n",
        "    brent = yf.download('BZ=F', start = start_date, end = end_date)['Adj Close']\n",
        "    moex = yf.download('IMOEX.ME', start = start_date, end = end_date)['Adj Close']\n",
        "\n",
        "    # Парсим ключевую ставку с официального сайта ЦБ\n",
        "    key_rate = pd.read_html('https://www.cbr.ru/hd_base/KeyRate/?UniDbQuery.Posted=True&UniDbQuery.From=09.01.2017&UniDbQuery.To=31.03.2021')[0]\n",
        "    key_rate['Дата'] = pd.to_datetime(key_rate['Дата'], format = \"%d.%m.%Y\")\n",
        "    key_rate['Key_Rate'] = key_rate['Ставка'] / 10000\n",
        "    key_rate.sort_values('Дата', ascending = True, inplace = True)\n",
        "    key_rate.index = key_rate['Дата']\n",
        "    key_rate.drop(['Ставка'], axis = 1, inplace = True)\n",
        "    key_rate.drop(['Дата'], axis = 1, inplace = True)\n",
        "\n",
        "    # Объединяем все данные в один датафрейм, а пропуски заменяем через линейную интерполяцию\n",
        "    self.data = pd.merge(left = self.data, right = usd_rub, how = 'left', left_on = self.data.index, right_on = usd_rub.index).set_index('key_0').rename(columns = {'Adj Close': 'USD_RUB'})\n",
        "    self.data = pd.merge(left = self.data, right = brent, how = 'left', left_on = self.data.index, right_on = brent.index).set_index('key_0').rename(columns = {'Adj Close': 'Brent'})\n",
        "    self.data = pd.merge(left = self.data, right = moex, how = 'left', left_on = self.data.index, right_on = moex.index).set_index('key_0').rename(columns = {'Adj Close': 'MOEX'})\n",
        "    self.data = pd.merge(left = self.data, right = key_rate, how = 'left', left_on = self.data.index, right_on = key_rate.index).set_index('key_0')\n",
        "    self.data.interpolate(method = 'linear', inplace = True)\n",
        "  \n",
        "    # Возвращаем датафрейм со всеми загруженными признаками\n",
        "    return self.data\n",
        "\n",
        "  def generate_features(self):\n",
        "\n",
        "    # Лаги зависимой переменной\n",
        "    self.data['Balance_lag_1'] = self.data['Balance'].shift(1)\n",
        "    self.data['Balance_lag_2'] = self.data['Balance'].shift(2)\n",
        "    self.data['Balance_lag_3'] = self.data['Balance'].shift(3)\n",
        "    self.data['Balance_lag_4'] = self.data['Balance'].shift(4)\n",
        "    self.data['Balance_lag_5'] = self.data['Balance'].shift(5)\n",
        "\n",
        "    # Скользящие средние зависимой переменной\n",
        "    self.data['Balance_Rolling_Mean_5'] = self.data['Balance'].rolling(window = 5).mean()\n",
        "    self.data['Balance_Rolling_Mean_15'] = self.data['Balance'].rolling(window = 15).mean()\n",
        "    self.data['Balance_Rolling_Mean_30'] = self.data['Balance'].rolling(window = 30).mean()\n",
        "    self.data['Balance_Rolling_Mean_45'] = self.data['Balance'].rolling(window = 45).mean()\n",
        "    self.data['Balance_Rolling_Mean_60'] = self.data['Balance'].rolling(window = 60).mean()\n",
        "\n",
        "    # Фиктивные переменные дня недели\n",
        "    self.data = pd.get_dummies(self.data, columns = ['Weekday'], drop_first = True)\n",
        "    self.data.dropna(inplace = True)\n",
        "\n",
        "    # Возвращаем датафрейм с загруженными признаками и сгенерированными из имеющихся рядов фичами\n",
        "    return self.data\n",
        "\n",
        "  def select_features(self):\n",
        "\n",
        "    # Разбиваем данные для использования библиотеки по отбору признаков\n",
        "    key_rate = self.data['Key_Rate'] \n",
        "    X = self.data.drop(['Balance'], axis = 1)\n",
        "    y = self.data['Balance']\n",
        "\n",
        "    # Обучаем модель для отбора лучших 5 признаков\n",
        "    X_kbest = SelectKBest(f_regression, k=5).fit_transform(X, y)\n",
        "    data_kbest_1 = pd.DataFrame(X_kbest, index = X.index)\n",
        "    data_kbest_1['Balance'] = y\n",
        "    data_kbest_1['Key_Rate'] = key_rate\n",
        "    self.data_kbest = data_kbest_1\n",
        "\n",
        "    # Обучаем PCA модель, сохраняя 90% вариации параметров\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X)\n",
        "    X_scaler = scaler.transform(X)\n",
        "    pca = PCA(.9)\n",
        "    pca.fit(X_scaler)\n",
        "    X_pca = pca.transform(X_scaler)\n",
        "    data_pca_1 = pd.DataFrame(X_pca, index = X.index)\n",
        "    data_pca_1['Balance'] = y\n",
        "    data_pca_1['Key_Rate'] = key_rate\n",
        "    self.data_pca = data_pca_1\n",
        "\n",
        "    # Возвращаем словарь с двумя датафреймами с выделенными параметрами\n",
        "    return {'data_kbest': self.data_kbest, 'data_pca': self.data_pca}\n",
        "\n",
        "  def best_arima_model(self, data):\n",
        "\n",
        "    # Разбиваем исходный датафрейм\n",
        "    key_rate = data['Key_Rate']\n",
        "    X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "    y = data['Balance']\n",
        "\n",
        "    # Ищем оптимальные параметры ARIMA\n",
        "    self.best_d = 0\n",
        "    y_temp = y\n",
        "    while adfuller(y_temp)[1] > 0.05:\n",
        "      self.best_d += 1\n",
        "      y_temp = y_temp.diff().dropna()\n",
        "    \n",
        "    self.best_p = 0\n",
        "    self.best_q = 0\n",
        "    best_errors_list = []\n",
        "    best_loss = 9999\n",
        "    start_index = 0\n",
        "    end_index = 500\n",
        "    step = 10\n",
        "    for p in tqdm(range(0, 1)):\n",
        "      for q in range(0, 1):\n",
        "\n",
        "        steps = list(range(end_index, len(X) - 1, 10))\n",
        "\n",
        "        errors_list = []\n",
        "        for i, step in enumerate(steps):\n",
        "          X_train = X.iloc[i*10 : step, :]\n",
        "          X_test = X.iloc[step : step+1, :]\n",
        "\n",
        "          y_train = y.iloc[i*10 : step]\n",
        "          y_test = y.iloc[step : step+1]\n",
        "\n",
        "          model = SARIMAX(y_train, X_train, order = (p, self.best_d, q))\n",
        "          result = model.fit()\n",
        "          y_pred = result.get_forecast(steps = 1, exog = np.array(X_test.iloc[-1]).reshape((1,-1))).predicted_mean\n",
        "\n",
        "          error = calc_loss(float(y_test), float(y_pred), key_rate.loc[y_test.index])\n",
        "          errors_list.append(abs(error))\n",
        "        \n",
        "        loss = np.mean(errors_list)\n",
        "        if loss < best_loss:\n",
        "          best_loss = loss\n",
        "          best_errors_list = errors_list\n",
        "          self.best_p = p\n",
        "          self.best_q = q\n",
        "\n",
        "    # Возвращаем оптимальную для ARIMA среднюю ошибку, максимальную ошибку в модели и массив гиперпараметров\n",
        "    return [best_loss, np.max(best_errors_list), 'ARIMA', [self.best_p, self.best_q]]\n",
        "\n",
        "  def best_xgb_model(self, data):\n",
        "\n",
        "    # Первичное деление готового датафрейма\n",
        "    key_rate = data['Key_Rate']\n",
        "    X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "    y = data['Balance']\n",
        "\n",
        "    # Определяем пространство возможных гиперпараметров\n",
        "    lr_list = np.linspace(1e-4, 1e-1, 2)\n",
        "    max_depth_list = np.array([3])\n",
        "    n_estimators_list = np.array([300])\n",
        "\n",
        "    # Ищем оптимальные параметры XGBoost\n",
        "    self.best_lr = 0\n",
        "    self.best_max_depth = 0\n",
        "    self.best_n_estimators = 0\n",
        "    best_errors_list = []\n",
        "    best_loss = 9999\n",
        "    start_index = 0\n",
        "    end_index = 500\n",
        "    step = 10\n",
        "    for lr in tqdm(lr_list):\n",
        "      for max_depth in max_depth_list:\n",
        "        for n_estimators in n_estimators_list:\n",
        "\n",
        "          steps = list(range(end_index, len(X) - 1, 10))\n",
        "\n",
        "          errors_list = []\n",
        "          for i, step in enumerate(steps):\n",
        "            X_train = X.iloc[i*10 : step, :]\n",
        "            X_test = X.iloc[step : step+1, :]\n",
        "\n",
        "            y_train = y.iloc[i*10 : step]\n",
        "            y_test = y.iloc[step : step+1]\n",
        "\n",
        "            model = xgb.XGBRegressor(learning_rate = lr, max_depth = max_depth, n_estimators = n_estimators, verbosity = 0)\n",
        "            result = model.fit(X_train, y_train)\n",
        "            y_pred = result.predict(X_test)\n",
        "\n",
        "            error = calc_loss(y_test[0], y_pred[0], key_rate.loc[y_test.index].values)\n",
        "            errors_list.append(abs(error))\n",
        "        \n",
        "        loss = np.mean(errors_list)\n",
        "        if loss < best_loss:\n",
        "          best_loss = loss\n",
        "          best_errors_list = errors_list\n",
        "          self.best_lr = lr\n",
        "          self.best_max_depth = max_depth\n",
        "          self.best_n_estimators = n_estimators\n",
        "\n",
        "    # Возвращаем оптимальную для XGboost среднюю ошибку, максимальную ошибку в модели и массив гиперпараметров\n",
        "    return [best_loss, np.max(best_errors_list), 'XGBoost', [self.best_lr, self.best_max_depth, self.best_n_estimators]]\n",
        "\n",
        "  def best_rf_model(self, data):\n",
        "\n",
        "    # Первичное деление готового датафрейма\n",
        "    key_rate = data['Key_Rate']\n",
        "    X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "    y = data['Balance']\n",
        "\n",
        "    # Определяем пространство возможных гиперпараметров\n",
        "    min_samples_split_list = np.array([4])\n",
        "    min_samples_leaf_list = np.array([5])\n",
        "    max_depth_list = np.array([4])\n",
        "    n_estimators_list = np.array([300])\n",
        "\n",
        "    # Ищем оптимальные параметры RandomForest\n",
        "    self.best_min_samples_split = 0\n",
        "    self.best_min_samples_leaf = 0\n",
        "    self.best_max_depth = 0\n",
        "    self.best_n_estimators = 0\n",
        "    best_errors_list = []\n",
        "    best_loss = 9999\n",
        "    start_index = 0\n",
        "    end_index = 500\n",
        "    step = 10\n",
        "    for min_samples_leaf in tqdm(min_samples_leaf_list):\n",
        "      for min_samples_split in min_samples_split_list:\n",
        "        for max_depth in max_depth_list:\n",
        "          for n_estimators in n_estimators_list:\n",
        "\n",
        "            steps = list(range(end_index, len(X) - 1, 10))\n",
        "\n",
        "            errors_list = []\n",
        "            for i, step in enumerate(steps):\n",
        "              X_train = X.iloc[i*10 : step, :]\n",
        "              X_test = X.iloc[step : step+1, :]\n",
        "\n",
        "              y_train = y.iloc[i*10 : step]\n",
        "              y_test = y.iloc[step : step+1]\n",
        "\n",
        "              model = RandomForestRegressor(min_samples_leaf = min_samples_leaf,\n",
        "                                            min_samples_split = min_samples_split,\n",
        "                                            max_depth = max_depth, \n",
        "                                            n_estimators = n_estimators, \n",
        "                                            verbose = 0)\n",
        "              \n",
        "              result = model.fit(X_train, y_train)\n",
        "              y_pred = result.predict(X_test)\n",
        "\n",
        "              error = calc_loss(y_test[0], y_pred[0], key_rate.loc[y_test.index].values)\n",
        "              errors_list.append(abs(error))\n",
        "          \n",
        "            loss = np.mean(errors_list)\n",
        "            if loss < best_loss:\n",
        "              best_loss = loss\n",
        "              best_errors_list = errors_list\n",
        "              self.best_min_samples_leaf = min_samples_leaf\n",
        "              self.best_min_samples_split = min_samples_split\n",
        "              self.best_max_depth = max_depth\n",
        "              self.best_n_estimators = n_estimators\n",
        "    \n",
        "    # Возвращаем оптимальную для RandomForest среднюю ошибку, максимальную ошибку в модели и массив гиперпараметров\n",
        "    return [best_loss, np.max(best_errors_list), 'RandomForest', [self.best_min_samples_leaf, self.best_min_samples_split, self.best_max_depth, self.best_n_estimators]]\n",
        "\n",
        "  def find_best_model(self, data, start_index = 0, end_index = 500, step = 10):\n",
        "\n",
        "    # Запускаем поиск оптимальных гиперпараметров ARIMA, XGB и RF моделей\n",
        "    best_arima = self.best_arima_model(data)\n",
        "    best_xgb = self.best_xgb_model(data)\n",
        "    best_rf = self.best_rf_model(data)\n",
        "\n",
        "    best_models = [best_arima, best_xgb, best_rf]\n",
        "\n",
        "    # Определяем лучшую модель из трех лучших\n",
        "    best_score = 9999\n",
        "    best_index = 0\n",
        "    for i, model in enumerate(best_models):\n",
        "      model_score = model[0]\n",
        "      if model_score > best_score:\n",
        "        best_score = model_score\n",
        "        best_index = i\n",
        "    \n",
        "    # Возвращаем характеристики лучшей модели\n",
        "    return best_models[best_index]\n",
        "\n",
        "  def activate_pipeline(self, start_index = 0, end_index = 500, step = 10):\n",
        "    \n",
        "    # Определяем интервал для обучения при backtesting\n",
        "    self.start_index = start_index\n",
        "    self.end_index = end_index\n",
        "    self.step = step\n",
        "\n",
        "    # Проводим первичные операции над исходным датафреймом\n",
        "    self.data = self.prepare_data()\n",
        "    self.data = self.generate_features()\n",
        "    \n",
        "    # Производим отбор признаков через два метода, определяем лучшие модели и лучший вариант отбора\n",
        "    dict_df = self.select_features()\n",
        "    best_loss = 9999\n",
        "    best_model = None\n",
        "    for df_selected in dict_df.keys():\n",
        "      self.best_model = self.find_best_model(dict_df[df_selected])\n",
        "      if self.best_model[0] < best_loss:\n",
        "        self.best_selected_df = dict_df[df_selected]\n",
        "        best_loss = self.best_model[0]\n",
        "        best_model = self.best_model\n",
        "\n",
        "    # Возвращаем характеристики лучшей модели и датафрейм с отобранными эффективным способом признаками\n",
        "    return [self.best_model, self.best_selected_df]\n",
        "  \n",
        "  def predict_1_day(self, prediction_date = None):\n",
        "    \n",
        "    # Если не указана дата желаемого прогноза\n",
        "    if prediction_date is None:\n",
        "\n",
        "      # Если дата, следующая за последним индексом датафрейма попадает на выходной день\n",
        "      if ((self.best_selected_df.index[-1] + timedelta(days=1)).dayofweek + 1 == 6) or \\\n",
        "         ((self.best_selected_df.index[-1] + timedelta(days=1)).dayofweek + 1 == 7):\n",
        "\n",
        "        return 0\n",
        "    \n",
        "      else:\n",
        "        data = self.best_selected_df\n",
        "        key_rate = data['Key_Rate']\n",
        "        X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "        y = data['Balance']\n",
        "\n",
        "        X.loc[X.index[-1] + timedelta(days = 1)] = [np.nan] * X.shape[1]\n",
        "        X.interpolate(method = 'linear', inplace = True)\n",
        "\n",
        "        X_train = X.iloc[:-1, :]\n",
        "        X_test = X.iloc[-1:, :]\n",
        "\n",
        "        y_train = y\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 2 гиперпараметра - это ARIMA модель\n",
        "        if len(self.best_model[3]) == 2:\n",
        "          model = SARIMAX(y_train, X_train, order = (self.best_p, self.best_d, self.best_q))\n",
        "          result = model.fit()\n",
        "          y_pred = result.get_forecast(steps = 1, exog = np.array(X_test.iloc[-1]).reshape((1,-1))).predicted_mean\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 3 гиперпараметра - это XGBoost модель\n",
        "        elif len(self.best_model[3]) == 3:\n",
        "          model = xgb.XGBRegressor(learning_rate = self.best_lr, max_depth = self.best_max_depth, n_estimators = self.best_n_estimators)\n",
        "          result = model.fit(X_train, y_train)\n",
        "          y_pred = result.predict(X_test)\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 4 гиперпараметра - это RandomForest модель\n",
        "        elif len(self.best_model[3]) == 4:\n",
        "          model = RandomForestRegressor(min_samples_leaf = self.best_min_samples_leaf,\n",
        "                                              min_samples_split = self.best_min_samples_split,\n",
        "                                              max_depth = self.best_max_depth, \n",
        "                                              n_estimators = self.best_n_estimators, \n",
        "                                              verbose = 0)\n",
        "                \n",
        "          result = model.fit(X_train, y_train)\n",
        "          y_pred = result.predict(X_test)\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Настраиваем дату в нужный формат. Предварительно даты заносятся в виде 'dd.mm.YYYY'\n",
        "      prediction_date = pd.to_datetime(prediction_date, format = '%d.%m.%Y')\n",
        "\n",
        "      # Если указанная дата попала на выходной день\n",
        "      if (prediction_date.dayofweek + 1 == 6) or (prediction_date.dayofweek + 1 == 7):\n",
        "        return 0\n",
        "      \n",
        "      # Если указанная дата представляет собой день, следующий за последним индексом датафрейма\n",
        "      elif prediction_date == self.best_selected_df.index[-1] + timedelta(days=1):    \n",
        "        data = self.best_selected_df\n",
        "        key_rate = data['Key_Rate']\n",
        "        X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "        y = data['Balance']\n",
        "\n",
        "        X.loc[X.index[-1] + timedelta(days = 1)] = [np.nan] * X.shape[1]\n",
        "        X.interpolate(method = 'linear', inplace = True)\n",
        "\n",
        "        X_train = X.iloc[:-1, :]\n",
        "        X_test = X.iloc[-1:, :]\n",
        "\n",
        "        y_train = y\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 2 гиперпараметра - это ARIMA модель\n",
        "        if len(self.best_model[3]) == 2:\n",
        "          model = SARIMAX(y_train, X_train, order = (self.best_p, self.best_d, self.best_q))\n",
        "          result = model.fit()\n",
        "          y_pred = result.get_forecast(steps = 1, exog = np.array(X_test.iloc[-1]).reshape((1,-1))).predicted_mean\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 3 гиперпараметра - это XGBoost модель\n",
        "        elif len(self.best_model[3]) == 3:\n",
        "          model = xgb.XGBRegressor(learning_rate = self.best_lr, max_depth = self.best_max_depth, n_estimators = self.best_n_estimators)\n",
        "          result = model.fit(X_train, y_train)\n",
        "          y_pred = result.predict(X_test)\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "        # Ситуация, когда в лучшей модели 4 гиперпараметра - это RandomForest модель\n",
        "        elif len(self.best_model[3]) == 4:\n",
        "          model = RandomForestRegressor(min_samples_leaf = self.best_min_samples_leaf,\n",
        "                                              min_samples_split = self.best_min_samples_split,\n",
        "                                              max_depth = self.best_max_depth, \n",
        "                                              n_estimators = self.best_n_estimators, \n",
        "                                              verbose = 0)\n",
        "                \n",
        "          result = model.fit(X_train, y_train)\n",
        "          y_pred = result.predict(X_test)\n",
        "          if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "            return y_pred[0]\n",
        "          else:\n",
        "            return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "      else:\n",
        "        data = self.best_selected_df\n",
        "        key_rate = data['Key_Rate']\n",
        "        X = data.drop(['Balance', 'Key_Rate'], axis = 1)\n",
        "        y = data['Balance']\n",
        "\n",
        "        # Когда дата прогноза есть в индексе в имеющимся датафрейме\n",
        "        if prediction_date in X.index:\n",
        "          \n",
        "          # Модель обучается до момента даты прогноза\n",
        "          X_train = X.loc[:(prediction_date + timedelta(days=1)), :]\n",
        "          X_test = X.loc[prediction_date:prediction_date, :]\n",
        "\n",
        "          # Модель прогнозируется на следующее наблюдение\n",
        "          y_train = y.loc[:(prediction_date + timedelta(days=1))]\n",
        "          y_test = y.loc[prediction_date:prediction_date]\n",
        "\n",
        "          # Ситуация, когда в лучшей модели 2 гиперпараметра - это ARIMA модель\n",
        "          if len(self.best_model[3]) == 2:\n",
        "            model = SARIMAX(y_train, X_train, order = (self.best_p, self.best_d, self.best_q))\n",
        "            result = model.fit()\n",
        "            y_pred = result.get_forecast(steps = 1, exog = np.array(X_test.iloc[-1]).reshape((1,-1))).predicted_mean\n",
        "            if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "              return y_pred[0]\n",
        "            else:\n",
        "              return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "          # Ситуация, когда в лучшей модели 3 гиперпараметра - это XGBoost модель\n",
        "          elif len(self.best_model[3]) == 3:\n",
        "            model = xgb.XGBRegressor(learning_rate = self.best_lr, max_depth = self.best_max_depth, n_estimators = self.best_n_estimators)\n",
        "            result = model.fit(X_train, y_train)\n",
        "            y_pred = result.predict(X_test)\n",
        "            if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "              return y_pred[0]\n",
        "            else:\n",
        "              return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "          # Ситуация, когда в лучшей модели 4 гиперпараметра - это RandomForest модель\n",
        "          elif len(self.best_model[3]) == 4:\n",
        "            model = RandomForestRegressor(min_samples_leaf = self.best_min_samples_leaf,\n",
        "                                                min_samples_split = self.best_min_samples_split,\n",
        "                                                max_depth = self.best_max_depth, \n",
        "                                                n_estimators = self.best_n_estimators, \n",
        "                                                verbose = 0)\n",
        "                  \n",
        "            result = model.fit(X_train, y_train)\n",
        "            y_pred = result.predict(X_test)\n",
        "            if y_pred[0] < y_train.quantile(0.95) and y_pred[0] > y_train.quantile(0.05):\n",
        "              return y_pred[0]\n",
        "            else:\n",
        "              return f'Прогноз {y_pred[0]} является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности'\n",
        "\n",
        "        else:\n",
        "          return 'Введенная дата недоступна для прогноза'"
      ],
      "metadata": {
        "id": "blHukgqzDmI6"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка работоспособности алгоритма"
      ],
      "metadata": {
        "id": "1mFtXjhjUhdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем экземпляр класса с нашим исходным датафреймом\n",
        "example = Pipeline(pd.read_excel('/content/Project 2_2022.xlsx', parse_dates = ['Date'], index_col = 'Date'))"
      ],
      "metadata": {
        "id": "oAM4ugqDXjLc"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Активируем пайплайн\n",
        "result = example.activate_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfAL9V7-u3_d",
        "outputId": "d051616f-687a-479b-976c-7e08f5c7356f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [27:17<00:00, 409.36s/it]\n",
            "100%|██████████| 20/20 [14:03<00:00, 42.15s/it]\n",
            "100%|██████████| 3/3 [30:03<00:00, 601.02s/it]\n",
            "100%|██████████| 4/4 [30:30<00:00, 457.54s/it]\n",
            "100%|██████████| 20/20 [21:29<00:00, 64.48s/it]\n",
            "100%|██████████| 3/3 [36:34<00:00, 731.51s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим на результат, где первый элемент массива - параметры лучшей модели (средняя абсолютная ошибка, \n",
        "# максимальная абсолютная ошибка, название модели и массив гиперпарамтеров), а второй - датафрейм с отобранными признаками\n",
        "\n",
        "print('Средняя абсолютная ошибка:', result[0][0])\n",
        "print('Максимальная абсолютная ошибка:', result[0][1])\n",
        "print('Название модели:', result[0][2])\n",
        "\n",
        "if result[0][2] == 'ARIMA':\n",
        "  print('Параметр p ARIMA модели:', result[0][3][0])\n",
        "  print('Параметр q ARIMA модели:', result[0][3][1])\n",
        "  \n",
        "elif result[0][2] == 'XGBoost':\n",
        "  print('Параметр learning_rate XGBoost модели:', result[0][3][0])\n",
        "  print('Параметр max_depth XGBoost модели:', result[0][3][1])\n",
        "  print('Параметр n_estimators XGBoost модели:', result[0][3][2])\n",
        "\n",
        "elif result[0][2] == 'RandomForest':\n",
        "  print('Параметр min_samples_leaf RandomForest модели:', result[0][3][0])\n",
        "  print('Параметр min_samples_split RandomForest модели:', result[0][3][1])\n",
        "  print('Параметр max_depth RandomForest модели:', result[0][3][2])\n",
        "  print('Параметр n_estimators RandomForest модели:', result[0][3][3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSp_NHkUTJzC",
        "outputId": "150e4414-9cf5-4163-cf49-bbf76ba7ea1f"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Средняя абсолютная ошибка: 0.00834836577989709\n",
            "Максимальная абсолютная ошибка: 0.09540074493647305\n",
            "Название модели: ARIMA\n",
            "Параметр p ARIMA модели: 0\n",
            "Параметр q ARIMA модели: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Как видим, лучше всего себя во время backtesting проявила модель MA(3). Для нее средняя абсолютная ошибка нашей метрики равняется 0.008, а максимальная ошибка - около 0.1, что удовлетворяет потребностям бизнеса"
      ],
      "metadata": {
        "id": "IsYT7o_kKgcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пробуем разные прогнозы\n",
        "print('Прогноз на 1 день вперед:', example.predict_1_day())\n",
        "print('Прогноз на произвольный будний день внутри выборки:', example.predict_1_day('22.04.2020'))\n",
        "print('Прогноз на произвольный будний день вне выборки:', example.predict_1_day('24.02.2022'))\n",
        "print('Прогноз на произвольный выходной день внутри выборки:', example.predict_1_day('25.02.2020'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYL7gHYAWXmL",
        "outputId": "df0860f2-7a0e-4779-c70e-11b4d21e32eb"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Прогноз на 1 день вперед: Прогноз -0.9136592714463844 является неустойчивым относительно выборки, требуется ручная настройка и анализ потоков ликвидности\n",
            "Прогноз на произвольный будний день внутри выборки: -0.12105507380352797\n",
            "Прогноз на произвольный будний день вне выборки: Введенная дата недоступна для прогноза\n",
            "Прогноз на произвольный выходной день внутри выборки: -0.08112575553237784\n"
          ]
        }
      ]
    }
  ]
}